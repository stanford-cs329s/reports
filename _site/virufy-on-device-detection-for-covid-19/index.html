<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Virufy: On-Device Detection for COVID-19</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="CS 329S Winter 2021 Reports" property="og:site_name">
  
    <meta content="Your Project Title" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="Your report description" property="og:description">
  
  
    <meta content="https://stanford-cs329s.github.io//template-report/" property="og:url">
  
  
    <meta content="2021-03-12T00:32:20-06:00" property="article:published_time">
    <meta content="https://stanford-cs329s.github.io//about/" property="article:author">
  
  
    <meta content="https://stanford-cs329s.github.io//reports/assets/img/stanfordlogo.png" property="og:image">
  
  
    
  
  
    
    <meta content="Edge-ML" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@chipro">
    <meta name="twitter:creator" content="@chipro">
  
    <meta name="twitter:title" content="Your Project Title">
  
  
    <meta name="twitter:url" content="https://stanford-cs329s.github.io//template-report/">
  
  
    <meta name="twitter:description" content="Your report description">
  
  
    <meta name="twitter:image:src" content="https://stanford-cs329s.github.io//reports/assets/img/stanfordlogo.png">
  

	<meta name="description" content="Your report description">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/reports/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/reports/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/reports/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/reports/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700|Lato:300,400,700&display=swap" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/reports/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/reports/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    
 
<aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/reports/"><img src="/reports/assets/img/stanfordlogo.png" alt="CS 329S"></a>
      </div>
      <div class="author-name">CS 329S</div>
      <p>Machine Learning Systems Design</p>
    </div>
  </header> <!-- End Header -->

  <ul class="tags">
    
      <li >
        <a href="/reports/tags/#Edge-ML" style="font-size: 13px;" class="tag">Edge-ML
          <span>(2)</span>
        </a>
      </li>
    
  </ul>

  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact us</h3>
      <ul>
        
          <li><a href="https://twitter.com/chipro" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/chiphuyen" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/chiphuyen" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="email"><a href="mailto:chip@huyenchip.com"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2021 &copy; CS 329S</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Virufy: On-Device Detection for COVID-19</h1>
        <div class="page-date"><span>2021, Mar 18&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>

</p>

<p>
</p>
<p>
<strong>CS 329S: Machine Learning Systems Design</strong>
</p>
<p>
<strong>Stanford University / Winter 2021</strong>
</p>
<h1>Virufy: On-Device Detection for COVID-19</h1>


<p><strong>Solomon Kim, Vivian Chen, Daniel Tan, Amil Khanzada</strong>
</p>
<p><strong>March 18th, 2021</strong>
</p>
<h1><strong>Problem Description</strong></h1>


<p>
COVID-19 testing is inadequate, especially in developing countries. Testing is scarce, requires trained nurses with costly equipment, and is expensive, limiting how many people can obtain their results. Also, many people in developing countries cannot risk taking tests because results are not anonymous, and a positive result may mean a loss of day-to-day work income and starvation for their families, which further allows COVID-19 to spread. 
</p>
<p>
Numerous attempts have been made to solve this problem with partial success, including contact tracing apps which have not been widely adopted often due to privacy concerns. Pharmaceutical companies have also fast-tracked development of vaccines, but they still will not be widely available in developing countries for some time.
</p>
<p>
To combat these problems, we propose a free smartphone app to detect COVID-19 from cough recordings through machine learning analysis of audio signals, which would allow for mass-scale testing and could effectively stop the spread of the virus.
</p>
<p>
We decided to use offline edge prediction for our app for several reasons. Especially in developing countries, Internet connectivity / latency is limited and people often face censoring. Data privacy regulations such as GDPR are now commonplace and on-device prediction will allow for diagnoses without personal information or health data crossing borders. Because our app will potentially serve billions of predictions daily, edge prediction is also more cost-effective, as maintaining and scaling cloud infrastructure to serve all of these predictions will be costly and difficult to maintain.
</p>
<h1>System Design</h1>


<p>
In designing our system and pipeline, we first and foremost kept in mind that this pipeline would be running offline on edge devices in developing countries, including outdated phones with weak CPUs. We aimed for a pipeline that could efficiently process data, run a simple model, and return a prediction within a minute. To do this, we simplified our model, sacrificing some “expressiveness” in exchange for reduced complexity, but also through straightforward preprocessing of data.
</p>
<p>
For the frontend, we decided on a web app because it can be used in the browser, which is operating-system-agnostic; in comparison, apps may only run on certain operating systems. Our frontend is written in <a href="https://www.typescriptlang.org/docs/handbook/react.html">ReactJS + TypeScript</a>, which is the industry standard for modern web design. It employs responsive web design principles to be compatible with a wide range of screen sizes and aspect ratios present on different devices. Internally, the frontend calls a <a href="https://www.tensorflow.org/js">TensorFlow.js</a> (TFJS) model for inference.
</p>
<p>

<div style="text-align:center;">
    <img src="/reports/assets/img/image1.png" alt="system_design" title="image_tooltip">
</div>


</p>
<p>
We chose to use the <a href="https://www.tensorflow.org/js">TensorFlow.js</a> (TFJS) framework because it is supported for use with web browsers. The TFJS <a href="https://github.com/tensorflow/tfjs-models/tree/master/speech-commands">Speech Command</a> library provides a JavaScript implementation of the Fourier Transform (Browser FFT) to allow straightforward preprocessing of the raw audio files. We trained a vanilla TensorFlow model on background noise examples provided by the sample TFJS Speech Commands code, along with a dataset of thousands COVID-19 test result labeled coughs, so that our model could distinguish coughs from background noise. We then converted this trained model into the TFJS LayersModel format (with the model architecture as a JSON and weights in .bin files), so that we could integrate it into the front end JavaScript code for browser inference on-device.
</p>
<p>
Our system’s basic pipeline is as follows:
</p>
<ol>

<li>User opens our app

<li>The TFJS models are downloaded from S3 onto the user’s device

<li>Microphone detects noise from user

<li>The Speech Commands library continuously preprocesses the audio by creating audio spectrograms

<li>The spectrograms are run through the model

<li>Only if the audio snippet is classified as a cough, the user will receive a prediction of whether they are COVID positive or negative
</li>
</ol>
<p>
It is worth noting that model files are downloaded and loaded into memory only when the user first opens the app. After this, no Internet access is required and the system is able to make predictions offline.
</p>
<h1>Machine Learning Component</h1>


<p>
The model that powers our application is based on the publicly available TensorFlow.js <a href="https://github.com/tensorflow/tfjs-models/tree/master/speech-commands">Speech Commands</a> module. Our model is intended to be used with the WebAudio API supported by all major browsers and expects, as input, audio data preprocessed with the browser Fast Fourier Transform (FFT) used in WebAudio’s GetFloatFrequencyData. The result of this FFT is a spectrogram which represents a sound wave as a linear combination of single-frequency components. The spectrogram, which can be thought of as a 2D image, is passed through a convolutional architecture to obtain logits which can be used in multiclass prediction. Specifically, this model has 13 layers with four pairs of Conv2D to MaxPooling layers, two dropout layers, a flatten layer, and two dense layers.
</p>
<p>


<img src="/reports/assets/img/image7.png" width="100%" alt="machine_learning_component">

</p>
<p>
Because training from scratch is expensive, we started with a model trained using the Speech Commands dataset [2], trained to recognize 20 common words such as the numbers “one” to “ten”, the four directions “left”, “right”, “up”, “down”, and basic commands like “stop” and “go”. We performed transfer learning on this model by removing the prediction layer and initializing a new one with the correct number of prediction classes. Afterwards, we fine-tuned the weights on the open source <a href="https://zenodo.org/record/4498364#.YFQG6a9Kh3g">COUGHVID dataset</a>, which provides over 20,000 crowdsourced cough recordings from a plethora of different characteristics including gender, geographic location, age, and COVID status. 
</p>
<p>
To ensure that data is preprocessed in the same way during training and testing, we use a custom preprocessing TensorFlow model which is trained to emulate the browser FFT that is performed by WebAudio, producing the same spectrogram as output. This browser FFT emulating model is provided and maintained by Tensorflow Speech Commands.  Creating our own training pipeline allowed us to select our model architecture based on existing ongoing research efforts and fine tune our hyperparameters.
</p>
<h1>System Evaluation</h1>


<p>
Offline evaluation was done on our model as a quick way to ensure our model was working correctly. This meant setting aside 30% of our data as test data. To monitor offline testing, we used <a href="https://wandb.ai/site">Weights and Biases</a>. As shown below, 50 epochs were sufficient to achieve convergence in training and validation accuracies, with corresponding decreasing losses. Here is an example of what we we logged:
</p>

<img src="/reports/assets/img/image2.png" width="100%" alt="system_evalution">
<p>
As demonstrated by the graphs as well as the chart, the “loss”, or the loss calculated from our training set was 0.1717. While the ‘val_loss’, or the loss calculated from the testing set was 0.09781. Also, the “acc” or the accuracy calculated from the training set was 0.93298. While the ‘val_acc’ or the accuracy calculated from the testing set was 0.96875. Additionally, we evaluated the model before and after TFJS conversion and found that the accuracy as well as the loss on both the training and testing set were the same. This was important because we were initially concerned that during the conversion process the quality of our model would go down, however we were delightfully surprised that this did not occur.
</p>
<p>
The remainder of our evaluation was done through real world testing. Although the gold standard of testing would be large-scale, randomized clinical trials, with data collected from a variety of demographic groups and recording devices, we did not have the time and resources to do that in the constraints of the class. Instead, we did informal evaluations on our own team members and friends in Argentina and Brazil. 
</p>
<p>
Anecdotally, the prediction was highly accurate on our group members, who were primarily Asian and all healthy. This remained true across a variety of devices such as smartphones, laptops, and tablets. 
</p>
<p>
The collection of external results was complicated by ethical considerations and lack of access to PCR tests to provide ground-truth labels. Nonetheless, we will note here two cases in Brazil. One individual was recovered, but previously was diagnosed with COVID-19; the model predicted that he had COVID-19. The other individual had COVID-19, but was predicted to be healthy. This illustrates the inherent challenge of translating models from development to production; model accuracy might be highly degraded due to distribution shift between the training and inference data.
</p>
<h1>Application Demonstration</h1>


<p>
In the beginning stages of the design process prior to this course, the Virufy product designer determined the appropriate target audience by conducting user interviews. She selected potential interviewee candidates based on certain demographic criteria such as being a citizen of selected Latin America countries or being tech-savvy and owning a cell phone.
</p>
<p>
After gathering target audience candidates from six Latin America countries as well as the U.S. and Pakistan, user interviews were conducted. The results from the interviews were then synthesized to create user personas. These personas helped her produce empathetic and user-centered designs throughout the whole design process.
</p>
<p>


    <img src="/reports/assets/img/image3.png" width="100%" alt="application_demonstration">




</p>
<p>
Once initial ideation and designs were completed, the designer conducted a series of prototype user tests in which the user was observed as they walked themselves through the app mockup. The data from each user test was then synthesized to design new and improved iterations. After numerous user tests and iterations and evolving, the designer created a mockup of the demonstration application.
</p>
<p>
Over the past month, we worked with the Virufy designer to adapt the design to our specific user needs given our novel contribution towards edge prediction. Through discussions with hospitals and normal users, alongside the technical limitations of TensorFlow.js, we finalized on our below design in which the user could click the microphone to trigger our model execution. We made the instructions simple and easy to follow, so users could record their cough and immediately get their prediction with our edge model which performed very fast (under 200ms on our laptops).
</p>
<p>
Here is a demo of our application: 
<br></br>
<iframe width="100%" height="400" src="https://www.youtube.com/embed/YW7Kne4s0Iw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>
<h1>Reflections</h1>


<p>
Throughout the course of this 2-month project, we explored many areas technically, some of which were fruitful, and others of which were dead ends.
</p>
<ol>

<li><strong>Google’s Teachable Machine</strong>
<p>

    At the start of our project, we used the MFCC and mel-spectrogram audio features in our models based on state-of-the-art research, but ran into issues as the same preprocessing code was not supported on-device with TFJS. We reached out to <a href="https://petewarden.com/">Pete Warden</a>, an expert of <a href="https://www.tinyml.org/">TinyML</a> on Google’s TensorFlow team, who pointed us to Teachable Machine, a web-based tool to create models, which uses TensorFlow.js to train models and generates code to integrate into JavaScript front ends. Although very simple and lightweight, we soon discovered <a href="https://teachablemachine.withgoogle.com/">Teachable Machine</a> was not a feasible long-term solution for us, as it required manual recording and upload of training audio files, while also not providing us the flexibility to configure model architecture as we hoped to do. This ultimately forced us to train our own custom model. 
</p>
</li>
</ol>
<ol>

<li><strong>Speech Commands Library</strong>
<p>

    TensorFlow’s <a href="https://www.tensorflow.org/tutorials/audio/simple_audio">Speech Commands</a> library provided a simple API to access a variety of important features like segmenting the continuous audio stream into one-second snippets and performing FFT feature extraction to obtain spectrograms. The availability of pre-existing training pipelines as well as example applications using Speech Commands provided a strong foundation for us to adapt our own pipeline and frontend application. 
</p>
<ol>

<li><strong>Team Dynamics</strong>
<p>

    We compartmentalized responsibilities such that individual members were largely in charge of separate components of the system. Frequent communication via Slack was key to ensure that we all had a sense of the bigger picture.
</p>
</li>
</ol>
</li>
</ol>
<p>
Overall, we learned over the quarter how to integrate frontend and backend codebases to build a production machine learning system, while utilizing APIs and libraries to expedite the process. Our knowledge also broadened as we considered the unique challenges of developing models for CPU-bound edge devices in the audio analysis domain.
</p>
<p>
Continuing beyond this course, we would like to explore the following areas:
</p>
<ol>

<li><strong>Model Performance</strong>
<p>

    State-of-the-art <a href="https://virufy.org/paper">research papers</a> suggest that accuracies as high as 98% are possible for larger neural networks. We would like to tune our tiny edge models to perform at similar accuracies.
</p>
<ol>

<li><strong>Dataset Diversity</strong>
<p>

    Our model development was limited by the lack of access to large-scale, demographically diverse, and accurately labelled datasets. For next steps, we hope to remedy this by leveraging the <a href="https://github.com/iiscleap/Coswara-Data">Coswara dataset</a>, along with the larger datasets Virufy is collecting globally.
</p>
<ol>

<li><strong>Microphone Calibration</strong>
<p>

    We didn’t take into account the distribution shift between training and inference due to differences of microphone hardware specifications between edge devices.
</p>
<ol>

<li><strong>Audio Compression</strong>
<p>

    The audio samples we trained on were of similar audio formats and frequencies. Exploring the effect of audio compression codecs such as mp3 on model performance may lead to interesting insights.
</p>
<ol>

<li><strong>Expansion to More Diseases</strong>
<p>

    COVID-19 is not the only disease that affects patient cough sounds. We believe our model can be enhanced to distinguish between various other coronaviruses such as the common cold and flu, along with asthma and pneumonia through use of a multi-class classifier.
</p>
<ol>

<li><strong>Embedded Hardware</strong>
<p>

    An interesting area to explore is further shrinking our model to fit onto specialized embedded devices with microphones. Such devices could be cheaply produced and shipped globally to provide COVID detection without smartphones.
</p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h1>Broader Impacts</h1>


<p>
Our app is intended to be used by people in developing countries who need an anonymous solution for testing anytime, or by anyone in a community at risk of COVID-19. However, we have identified some unintended uses of our app.
</p>
<p>
Because we intend to share our technology freely and because the algorithm runs on-device, competitors will easily be able to take our algorithm and create copies of our app and may even block access to our app and sell theirs for profit. To prevent this, we will open source our technology under terms requiring attribution to Virufy and prohibiting charging users for the use of the algorithm.
</p>
<p>
Another risk is that people may begin to ignore medical advice and believe only in the algorithm and might use the results in place of an actual diagnostic test. This is very risky because if the algorithm mispredicts, we may be held liable. The spread of COVID-19 may increase if COVID-19 positive people become confident to socialize with false negative test results. To mitigate this, we intend to add disclaimers that our app is a pre-screening tool that should be used only in conjunction with medical providers’ input. Additionally, we will work closely with public health authorities to clinically validate our algorithm and ensure it is safe for usage.
</p>
<p>

<div style="display:flex;">
    <img src="/reports/assets/img/image5.png" width="50%" alt="alt_text">

    <img src="/reports/assets/img/image6.png" width="50%" alt="alt_text">  
</div>


</p>
<p>
People may also start testing the algorithm with irrelevant recordings of random noises such as talking. To address this, we have equipped our algorithm with a cough detection pre-check layer to prevent any non-cough noises from being classified.
</p>
<p>
Finally, people especially in poorer contexts may share the same smartphones with several users, which can increase the likelihood of spreading COVID-19. Thus, our instructions clearly state that users must disinfect their device and keep 20 feet away from others while recording.
</p>
<h1>Code</h1>


<p>
Our TensorFlow JavaScript audio preprocessing and model prediction code can be found here: <a href="https://github.com/dtch1997/virufy-tm-cough-app">https://github.com/dtch1997/virufy-tm-cough-app</a>
</p>
<p>
Our finalized progressive web application code can be found here: <a href="https://github.com/virufy/demo/tree/edge-xoor">https://github.com/virufy/demo/tree/edge-xoor</a>
</p>
<h1>References</h1>


<p>
We’re extremely grateful to <a href="https://petewarden.com/">Pete Warden</a>, <a href="http://www.jasonmayes.com/">Jason Mayes</a>, and <a href="https://www.linkedin.com/in/tiezhen/">Tiezhen Wang</a> from Google’s TensorFlow.js team for their kind guidance on TinyML concepts and usage of the <a href="https://www.tensorflow.org/datasets/catalog/speech_commands">speech_commands library</a>, both in class lecture and during the few weeks of our development.
</p>
<p>
<a href="https://www.linkedin.com/in/jonatan-jaskilioff-77075340/">Jonatan Jaskilioff</a> and the team at <a href="https://xoor.io/">XOOR</a> were very gracious to lend their support and guidance in integrating our JavaScript code into the <a href="https://virufy.org/demo">progressive web app</a> they had built pro bono for Virufy.
</p>
<p>
We are also indebted to the broader <a href="http://virufy.org/">Virufy</a> team for guiding us on the real-world applicability and challenges of our edge device prediction project. We leveraged their deep insights from their members distributed across 20 developing countries in formulating our problem statement. Additionally, we built on top of the open source <a href="https://virufy.org/demo">demo app</a> that they had built prior based on intentions for real-life usage, along with their prior <a href="https://virufy.org/paper">research findings</a> and <a href="https://github.com/virufy/covid">open source code</a> for our model training.
</p>
<p>
In preparing our final report, we are grateful to <a href="https://www.linkedin.com/in/colleen-wang-59a091205/">Colleen Wang</a> for her kind support in editing the content of our post, Virufy lead UX designer <a href="https://www.linkedin.com/in/maisiemora/">Maisie Mora</a> for helping explain the design process in the application demonstration section, and <a href="https://www.linkedin.com/in/saslam23/">Saad Aslam</a> for his kind support in converting our blog post to a nicely formatted HTML page.
</p>
<p>
Finally, we cannot forget the great lessons and close guidance from Professor <a href="https://huyenchip.com/">Chip Huyen</a> and TA <a href="https://michaeljohncooper.com/">Michael Cooper</a> who helped us open our eyes to production machine learning and formulate our problem to be attainable within the short 2 month course quarter.
</p>
<p>
[1] Tensorflow Speech Commands dataset, <a href="https://arxiv.org/pdf/1804.03209.pdf">https://arxiv.org/pdf/1804.03209.pdf</a>
</p>
<p>
[2] Teachable Machine, <a href="https://teachablemachine.withgoogle.com/">https://teachablemachine.withgoogle.com/</a>
</p>
<p>
[3] Virufy: A Multi-Branch Deep Learning Network for Automated Detection of COVID-19 <a href="https://arxiv.org/ftp/arxiv/papers/2103/2103.01806.pdf">https://arxiv.org/ftp/arxiv/papers/2103/2103.01806.pdf</a>
</p>

</aside>
</div>
</body>
</html>